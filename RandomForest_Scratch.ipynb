{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33ca7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Node class used to build decision tree structure\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Represents a single node in the decision tree.\n",
    "\n",
    "    Attributes:\n",
    "        feature (str): The feature used for splitting at this node.\n",
    "        threshold (float or int): The threshold value for splitting.\n",
    "        left (Node): The left child node.\n",
    "        right (Node): The right child node.\n",
    "        value (any): The value at the leaf node (if applicable).\n",
    "    \"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        \"\"\"\n",
    "        Check if the node is a leaf node.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the node is a leaf node, False otherwise.\n",
    "        \"\"\"\n",
    "        return self.value is not None\n",
    "\n",
    "# --- Helper functions for classification and regression ---\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a target array.\n",
    "\n",
    "    Args:\n",
    "        y (array-like): The target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The entropy of the target values.\n",
    "    \"\"\"\n",
    "    counts = Counter(y)\n",
    "    total = len(y)\n",
    "    ent = 0.0\n",
    "    for count in counts.values():\n",
    "        p = count / total\n",
    "        ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "def information_gain(parent, left, right):\n",
    "    \"\"\"\n",
    "    Calculate the information gain from a split.\n",
    "\n",
    "    Args:\n",
    "        parent (array-like): The parent node target values.\n",
    "        left (array-like): The left child node target values.\n",
    "        right (array-like): The right child node target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The information gain from the split.\n",
    "    \"\"\"\n",
    "    weight_left = len(left) / len(parent)\n",
    "    weight_right = len(right) / len(parent)\n",
    "    return entropy(parent) - (weight_left * entropy(left) + weight_right * entropy(right))\n",
    "\n",
    "def mse(y):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error of a target array.\n",
    "\n",
    "    Args:\n",
    "        y (array-like): The target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean squared error of the target values.\n",
    "    \"\"\"\n",
    "    mean = np.mean(y)\n",
    "    return np.mean((y - mean) ** 2)\n",
    "\n",
    "def mse_reduction(parent, left, right):\n",
    "    \"\"\"\n",
    "    Calculate the reduction in mean squared error from a split.\n",
    "\n",
    "    Args:\n",
    "        parent (array-like): The parent node target values.\n",
    "        left (array-like): The left child node target values.\n",
    "        right (array-like): The right child node target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The reduction in mean squared error from the split.\n",
    "    \"\"\"\n",
    "    weight_left = len(left) / len(parent)\n",
    "    weight_right = len(right) / len(parent)\n",
    "    return mse(parent) - (weight_left * mse(left) + weight_right * mse(right))\n",
    "\n",
    "def calculate_gain(y_parent, y_left, y_right, task='classification'):\n",
    "    \"\"\"\n",
    "    Calculate the gain (information gain or MSE reduction) from a split.\n",
    "\n",
    "    Args:\n",
    "        y_parent (array-like): The parent node target values.\n",
    "        y_left (array-like): The left child node target values.\n",
    "        y_right (array-like): The right child node target values.\n",
    "        task (str): The task type ('classification' or 'regression').\n",
    "\n",
    "    Returns:\n",
    "        float: The gain from the split.\n",
    "    \"\"\"\n",
    "    if len(y_left) == 0 or len(y_right) == 0:\n",
    "        return 0\n",
    "    return information_gain(y_parent, y_left, y_right) if task == 'classification' else mse_reduction(y_parent, y_left, y_right)\n",
    "\n",
    "# --- Simple Decision Tree (supports classification and regression) ---\n",
    "class SimpleDecisionTree:\n",
    "    \"\"\"\n",
    "    A simple implementation of a decision tree that supports both classification and regression.\n",
    "\n",
    "    Attributes:\n",
    "        task (str): The task type ('classification', 'regression', or 'auto').\n",
    "        max_depth (int): The maximum depth of the tree.\n",
    "        tree (Node or dict): The root node of the decision tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, task='auto', max_depth=3):\n",
    "        self.task = task\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the decision tree to the training data.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The feature matrix.\n",
    "            y (Series or array-like): The target values.\n",
    "        \"\"\"\n",
    "        y = pd.Series(y)\n",
    "        if self.task == 'auto':\n",
    "            self.task = 'classification' if y.dtype == 'object' or len(np.unique(y)) <= 10 else 'regression'\n",
    "        data = X.copy()\n",
    "        data['target'] = y\n",
    "        self.tree = self._build_tree(data, depth=0)\n",
    "\n",
    "    def _build_tree(self, data, depth):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The dataset at the current node.\n",
    "            depth (int): The current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "            Node or value: The constructed tree or leaf value.\n",
    "        \"\"\"\n",
    "        y = data['target']\n",
    "        if len(y.unique()) == 1:\n",
    "            return y.iloc[0]\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return y.mode()[0] if self.task == 'classification' else y.mean()\n",
    "        best_feature, best_value = self._find_best_split(data)\n",
    "        if best_feature is None:\n",
    "            return y.mode()[0] if self.task == 'classification' else y.mean()\n",
    "        if self.task == 'classification':\n",
    "            left = data[data[best_feature] == best_value]\n",
    "            right = data[data[best_feature] != best_value]\n",
    "        else:\n",
    "            left = data[data[best_feature] <= best_value]\n",
    "            right = data[data[best_feature] > best_value]\n",
    "        return {\n",
    "            'feature': best_feature,\n",
    "            'value': best_value,\n",
    "            'left': self._build_tree(left, depth + 1),\n",
    "            'right': self._build_tree(right, depth + 1)\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, data):\n",
    "        \"\"\"\n",
    "        Find the best feature and value to split the data.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The dataset to split.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The best feature and value to split on.\n",
    "        \"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature, best_value = None, None\n",
    "        for col in data.columns:\n",
    "            if col == 'target':\n",
    "                continue\n",
    "            for val in np.unique(data[col]):\n",
    "                if self.task == 'classification':\n",
    "                    left = data[data[col] == val]['target']\n",
    "                    right = data[data[col] != val]['target']\n",
    "                else:\n",
    "                    left = data[data[col] <= val]['target']\n",
    "                    right = data[data[col] > val]['target']\n",
    "                if len(left) == 0 or len(right) == 0:\n",
    "                    continue\n",
    "                gain = calculate_gain(data['target'], left, right, self.task)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = col\n",
    "                    best_value = val\n",
    "        return best_feature, best_value\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for the given data.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            Series: The predicted target values.\n",
    "        \"\"\"\n",
    "        return X.apply(lambda row: self._predict_row(row, self.tree), axis=1)\n",
    "\n",
    "    def _predict_row(self, row, node):\n",
    "        \"\"\"\n",
    "        Predict the target value for a single data point.\n",
    "\n",
    "        Args:\n",
    "            row (Series): The feature values of the data point.\n",
    "            node (Node or dict): The current node of the tree.\n",
    "\n",
    "        Returns:\n",
    "            any: The predicted target value.\n",
    "        \"\"\"\n",
    "        if not isinstance(node, dict):#\n",
    "            return node\n",
    "        val = row[node['feature']]\n",
    "        if self.task == 'classification':\n",
    "            return self._predict_row(row, node['left']) if val == node['value'] else self._predict_row(row, node['right'])\n",
    "        else:\n",
    "            return self._predict_row(row, node['left']) if val <= node['value'] else self._predict_row(row, node['right'])\n",
    "\n",
    "# --- Random Forest class (classification & regression) ---\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    A simple implementation of a Random Forest algorithm that supports both classification and regression.\n",
    "\n",
    "    Attributes:\n",
    "        n_estimators (int): The number of trees in the forest.\n",
    "        max_depth (int): The maximum depth of each tree.\n",
    "        task (str): The task type ('classification' or 'regression').\n",
    "        max_features (str or int or float): The maximum number of features to consider for splits ('sqrt', int, or float).\n",
    "        trees (list): The list of fitted trees and their selected features.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=10, max_depth=5, task='classification', max_features='sqrt'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.task = task\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def _get_bootstrap_sample(self, X, y):\n",
    "        \"\"\"\n",
    "        Create a bootstrap sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The feature matrix.\n",
    "            y (Series): The target values.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The bootstrap sample of features and target values.\n",
    "        \"\"\"\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        return X.iloc[indices], y.iloc[indices]\n",
    "\n",
    "    def _get_feature_subset(self, X):\n",
    "        \"\"\"\n",
    "        Select a random subset of features for splitting.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            array: The selected features.\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        if self.max_features == 'sqrt':\n",
    "            k = int(np.sqrt(n_features))\n",
    "        elif isinstance(self.max_features, float):\n",
    "            k = int(self.max_features * n_features)\n",
    "        elif isinstance(self.max_features, int):\n",
    "            k = self.max_features\n",
    "        else:\n",
    "            k = n_features\n",
    "        return np.random.choice(X.columns, size=k, replace=False)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Random Forest to the training data.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The feature matrix.\n",
    "            y (Series or array-like): The target values.\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            X_sample, y_sample = self._get_bootstrap_sample(X, y)\n",
    "            selected_features = self._get_feature_subset(X_sample)\n",
    "            tree = SimpleDecisionTree(task=self.task, max_depth=self.max_depth)\n",
    "            tree.fit(X_sample[selected_features], y_sample)\n",
    "            self.trees.append((tree, selected_features))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for the given data.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            array: The predicted target values.\n",
    "        \"\"\"\n",
    "        tree_preds = [tree.predict(X[features]).values for tree, features in self.trees]\n",
    "        tree_preds = np.array(tree_preds).T\n",
    "        if self.task == 'classification':\n",
    "            return np.array([Counter(row).most_common(1)[0][0] for row in tree_preds])\n",
    "        else:\n",
    "            return np.mean(tree_preds, axis=1)\n",
    "\n",
    "# --- Accuracy metric for classification tasks ---\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy score for classification tasks.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): The true target values.\n",
    "        y_pred (array-like): The predicted target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy score.\n",
    "    \"\"\"\n",
    "    return np.mean(np.array(y_true) == np.array(y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340c9341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['Male' 'Female' 'Female' ... 'Female' 'Male' 'Female']\n",
      "Actual: 1501      Male\n",
      "2586    Female\n",
      "2653    Female\n",
      "1055      Male\n",
      "705       Male\n",
      "         ...  \n",
      "2313    Female\n",
      "3214      Male\n",
      "2732    Female\n",
      "1926      Male\n",
      "4227    Female\n",
      "Name: gender, Length: 1001, dtype: object\n",
      "Accuracy: 0.9140859140859141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"DATA/gender_classification_v7.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df.iloc[:, :-2]  \n",
    "y = df.iloc[:, -1]  \n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForest(n_estimators=10, max_depth=None, task='classification')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#Accuracy\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(\"Actual:\", y_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc7727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
